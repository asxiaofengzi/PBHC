import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Optional, Tuple
import sys
import os

# æ·»åŠ PBHCé¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from humanoidverse.agents.mh_ppo.mh_ppo import MHPPO
from humanoidverse.agents.modules.ppo_modules import PPOActor, PPOCritic
from .motion_encoder import MotionEncoder, MotionEncoderLoss, MotionType
from .fusion_controller import FusionController
from .multimodal_env import MultimodalMotionTrackingEnv

class MultiExpertActor(nn.Module):
    """
    å¤šä¸“å®¶Actorç½‘ç»œ - æ¯ç§è¿åŠ¨ç±»å‹å¯¹åº”ä¸€ä¸ªä¸“å®¶ç­–ç•¥
    """
    
    def __init__(self, 
                 obs_dim_dict: Dict,
                 module_config_dict: Dict,
                 num_actions: int,
                 num_motion_types: int,
                 init_noise_std: float = 1.0,
                 shared_backbone: bool = True):
        super().__init__()
        
        self.num_actions = num_actions
        self.num_motion_types = num_motion_types
        self.shared_backbone = shared_backbone
        
        # å…±äº«ä¸»å¹²ç½‘ç»œï¼ˆå¯é€‰ï¼‰
        if shared_backbone:
            backbone_dims = module_config_dict['layer_config']['hidden_dims'][:-1]
            self.shared_backbone_net = self._build_backbone(
                obs_dim_dict['actor_obs'], backbone_dims
            )
            expert_input_dim = backbone_dims[-1]
        else:
            self.shared_backbone_net = None
            expert_input_dim = obs_dim_dict['actor_obs']
        
        # ä¸ºæ¯ç§è¿åŠ¨ç±»å‹åˆ›å»ºä¸“å®¶ç½‘ç»œ
        self.motion_experts = nn.ModuleList()
        for motion_type in range(num_motion_types):
            expert = self._build_expert_network(
                expert_input_dim, 
                module_config_dict['layer_config']['hidden_dims'][-1:],
                num_actions
            )
            self.motion_experts.append(expert)
        
        # è¿åŠ¨ç±»å‹é€‰æ‹©ç½‘ç»œ
        self.motion_selector = nn.Sequential(
            nn.Linear(obs_dim_dict['actor_obs'], 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, num_motion_types),
            nn.Softmax(dim=-1)
        )
        
        # åŠ¨ä½œå™ªå£°å‚æ•°
        self.std = nn.Parameter(init_noise_std * torch.ones(num_actions))
        self.distribution = None
        
        # ç¦ç”¨å‚æ•°éªŒè¯ä»¥æé«˜é€Ÿåº¦
        from torch.distributions import Normal
        Normal.set_default_validate_args = False
    
    def _build_backbone(self, input_dim: int, hidden_dims: List[int]) -> nn.Module:
        """æ„å»ºå…±äº«ä¸»å¹²ç½‘ç»œ"""
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_dim = hidden_dim
        
        return nn.Sequential(*layers)
    
    def _build_expert_network(self, input_dim: int, hidden_dims: List[int], output_dim: int) -> nn.Module:
        """æ„å»ºä¸“å®¶ç½‘ç»œ"""
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        return nn.Sequential(*layers)
    
    def forward_experts(self, obs: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­æ‰€æœ‰ä¸“å®¶
        
        Args:
            obs: (batch_size, obs_dim) è§‚æµ‹
            
        Returns:
            expert_actions: (batch_size, num_motion_types, num_actions) æ‰€æœ‰ä¸“å®¶çš„åŠ¨ä½œ
        """
        batch_size = obs.size(0)
        
        # å…±äº«ä¸»å¹²ç‰¹å¾æå–
        if self.shared_backbone_net is not None:
            shared_features = self.shared_backbone_net(obs)
        else:
            shared_features = obs
        
        # æ‰€æœ‰ä¸“å®¶å¹¶è¡Œæ¨ç†
        expert_outputs = []
        for expert in self.motion_experts:
            expert_output = expert(shared_features)
            expert_outputs.append(expert_output)
        
        # å †å ä¸“å®¶è¾“å‡º
        expert_actions = torch.stack(expert_outputs, dim=1)  # (batch_size, num_experts, num_actions)
        
        return expert_actions
    
    def select_motion_weights(self, obs: torch.Tensor) -> torch.Tensor:
        """é€‰æ‹©è¿åŠ¨æƒé‡"""
        motion_weights = self.motion_selector(obs)
        return motion_weights
    
    def update_distribution(self, obs: torch.Tensor, fusion_weights: Optional[torch.Tensor] = None):
        """æ›´æ–°åŠ¨ä½œåˆ†å¸ƒ"""
        # è·å–æ‰€æœ‰ä¸“å®¶çš„åŠ¨ä½œ
        expert_actions = self.forward_experts(obs)
        
        # å¦‚æœæ²¡æœ‰æä¾›èåˆæƒé‡ï¼Œä½¿ç”¨å†…éƒ¨é€‰æ‹©å™¨
        if fusion_weights is None:
            fusion_weights = self.select_motion_weights(obs)
        
        # åŠ æƒèåˆä¸“å®¶åŠ¨ä½œ
        fused_mean = torch.sum(expert_actions * fusion_weights.unsqueeze(-1), dim=1)
        
        # åˆ›å»ºåˆ†å¸ƒ
        from torch.distributions import Normal
        self.distribution = Normal(fused_mean, fused_mean * 0. + self.std)
    
    def act(self, obs: torch.Tensor, fusion_weights: Optional[torch.Tensor] = None, **kwargs):
        """é‡‡æ ·åŠ¨ä½œ"""
        self.update_distribution(obs, fusion_weights)
        return self.distribution.sample()
    
    def act_inference(self, obs: torch.Tensor, fusion_weights: Optional[torch.Tensor] = None):
        """æ¨ç†åŠ¨ä½œï¼ˆæ— å™ªå£°ï¼‰"""
        expert_actions = self.forward_experts(obs)
        
        if fusion_weights is None:
            fusion_weights = self.select_motion_weights(obs)
        
        fused_actions = torch.sum(expert_actions * fusion_weights.unsqueeze(-1), dim=1)
        return fused_actions
    
    @property
    def action_mean(self):
        return self.distribution.mean if self.distribution else None
    
    @property
    def action_std(self):
        return self.distribution.stddev if self.distribution else None
    
    @property
    def entropy(self):
        return self.distribution.entropy().sum(dim=-1) if self.distribution else None
    
    def get_actions_log_prob(self, actions: torch.Tensor):
        return self.distribution.log_prob(actions).sum(dim=-1) if self.distribution else None
    
    def reset(self, dones=None):
        pass

class MultimodalPPO(MHPPO):
    """
    å¤šæ¨¡æ€PPOç®—æ³• - æ‰©å±•MHPPOä»¥æ”¯æŒå¤šæ¨¡æ€è¿åŠ¨èåˆå­¦ä¹ 
    """
    
    def __init__(self, env: MultimodalMotionTrackingEnv, config, log_dir=None, device='cpu'):
        # ç¡®ä¿ç¯å¢ƒæ˜¯å¤šæ¨¡æ€ç¯å¢ƒ
        assert isinstance(env, MultimodalMotionTrackingEnv), "Environment must be MultimodalMotionTrackingEnv"
        
        super().__init__(env, config, log_dir, device)
        
        # å¤šæ¨¡æ€ç›¸å…³é…ç½®
        self.multimodal_config = config.get('multimodal', {})
        self.enable_expert_training = self.multimodal_config.get('enable_expert_training', True)
        self.encoder_learning_rate = self.multimodal_config.get('encoder_learning_rate', 1e-4)
        self.fusion_loss_weight = self.multimodal_config.get('fusion_loss_weight', 0.1)
        
        # é¢„è®­ç»ƒé˜¶æ®µæ ‡å¿—
        self.pretraining_phase = self.multimodal_config.get('start_with_pretraining', True)
        self.pretraining_iterations = self.multimodal_config.get('pretraining_iterations', 5000)
        self.current_phase = 'pretraining' if self.pretraining_phase else 'multimodal'
        
        # é‡æ–°è®¾ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨
        self._setup_multimodal_models()
    
    def _setup_multimodal_models(self):
        """è®¾ç½®å¤šæ¨¡æ€æ¨¡å‹"""
        # å¤šä¸“å®¶Actor
        self.actor = MultiExpertActor(
            obs_dim_dict=self.algo_obs_dim_dict,
            module_config_dict=self.config.module_dict.actor,
            num_actions=self.num_act,
            num_motion_types=len(MotionType),
            init_noise_std=self.config.init_noise_std,
            shared_backbone=self.multimodal_config.get('shared_backbone', True)
        ).to(self.device)
        
        # Criticä¿æŒä¸å˜
        self.critic = PPOCritic(
            self.algo_obs_dim_dict,
            self.config.module_dict.critic
        ).to(self.device)
        
        # è·å–è¿åŠ¨ç¼–ç å™¨å’Œèåˆæ§åˆ¶å™¨ï¼ˆä»ç¯å¢ƒä¸­ï¼‰
        self.motion_encoder = self.env.motion_encoder
        self.fusion_controller = self.env.fusion_controller
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.actor_learning_rate)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.critic_learning_rate)
        
        # è¿åŠ¨ç¼–ç å™¨ä¼˜åŒ–å™¨
        self.encoder_optimizer = optim.Adam(
            self.motion_encoder.parameters(), 
            lr=self.encoder_learning_rate
        )
        
        # èåˆæ§åˆ¶å™¨ä¼˜åŒ–å™¨
        self.fusion_optimizer = optim.Adam(
            self.fusion_controller.parameters(),
            lr=self.encoder_learning_rate
        )
        
        # æŸå¤±å‡½æ•°
        
    def _setup_models_and_optimizer(self):
        """é‡å†™åŸºç±»æ–¹æ³•ä»¥ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹"""
        # è®¾ç½®å¥–åŠ±å‡½æ•°æ•°é‡
        self.config.module_dict.critic['output_dim'][-1] = self.num_rew_fn
        
        # è°ƒç”¨å¤šæ¨¡æ€æ¨¡å‹è®¾ç½®
        self._setup_multimodal_models()
        
        print("ğŸ­ Multimodal Actor:", self.actor)
        print("ğŸ§  Critic:", self.critic)
        self.encoder_loss_fn = MotionEncoderLoss(
            recon_weight=self.multimodal_config.get('recon_weight', 1.0),
            kl_weight=self.multimodal_config.get('kl_weight', 0.1),
            classification_weight=self.multimodal_config.get('classification_weight', 0.5)
        )
    
    def _actor_rollout_step(self, obs_dict, policy_state_dict):
        """é‡å†™Actor rolloutæ­¥éª¤ä»¥æ”¯æŒå¤šä¸“å®¶"""
        # è·å–èåˆæƒé‡
        if hasattr(self.env, 'current_fusion_weights'):
            fusion_weights = self.env.current_fusion_weights
        else:
            fusion_weights = None
        
        # ä½¿ç”¨èåˆæƒé‡é‡‡æ ·åŠ¨ä½œ
        actions = self.actor.act(obs_dict["actor_obs"], fusion_weights)
        policy_state_dict["actions"] = actions
        
        # è®°å½•ä¸“å®¶ä¿¡æ¯
        if fusion_weights is not None:
            policy_state_dict["fusion_weights"] = fusion_weights
            
            # è®°å½•ä¸“å®¶åŠ¨ä½œï¼ˆç”¨äºåˆ†æï¼‰
            with torch.no_grad():
                expert_actions = self.actor.forward_experts(obs_dict["actor_obs"])
                policy_state_dict["expert_actions"] = expert_actions
        
        # å…¶ä»–ä¿¡æ¯
        action_mean = self.actor.action_mean.detach()
        action_sigma = self.actor.action_std.detach() 
        actions_log_prob = self.actor.get_actions_log_prob(actions).detach().unsqueeze(1)
        
        policy_state_dict["action_mean"] = action_mean
        policy_state_dict["action_sigma"] = action_sigma
        policy_state_dict["actions_log_prob"] = actions_log_prob
        
        return policy_state_dict
    
    def _update_motion_encoder(self, policy_state_dict) -> Dict[str, float]:
        """æ›´æ–°è¿åŠ¨ç¼–ç å™¨"""
        if not hasattr(self.env, '_extract_current_motion_data'):
            return {}
        
        # æå–è¿åŠ¨æ•°æ®
        motion_data = self.env._extract_current_motion_data()
        motion_types = self.env.motion_types_tensor
        
        # å‰å‘ä¼ æ’­
        encoding_result = self.motion_encoder(motion_data, motion_types)
        
        # è®¡ç®—æŸå¤±
        losses = self.encoder_loss_fn(
            original=motion_data,
            reconstructed=encoding_result['reconstructed'],
            mean=encoding_result['mean'],
            logvar=encoding_result['logvar'],
            motion_pred=encoding_result['motion_pred'],
            motion_type=motion_types
        )
        
        # åå‘ä¼ æ’­
        self.encoder_optimizer.zero_grad()
        losses['total_loss'].backward()
        nn.utils.clip_grad_norm_(self.motion_encoder.parameters(), self.max_grad_norm)
        self.encoder_optimizer.step()
        
        # è¿”å›æŸå¤±ä¿¡æ¯
        return {k: v.item() for k, v in losses.items()}
    
    def _update_fusion_controller(self, policy_state_dict) -> Dict[str, float]:
        """æ›´æ–°èåˆæ§åˆ¶å™¨"""
        if not hasattr(self.env, 'current_fusion_weights'):
            return {}
        
        # è·å–è§‚æµ‹å’Œå½“å‰çŠ¶æ€
        obs = policy_state_dict.get('actor_obs')
        current_latent = self.env.current_motion_latent
        
        if obs is None or current_latent is None:
            return {}
        
        # å‰å‘ä¼ æ’­èåˆæ§åˆ¶å™¨
        selection_result = self.fusion_controller.select_active_motions(obs, current_latent)
        fusion_weights = self.fusion_controller.compute_fusion_weights(
            obs, current_latent, selection_result['motion_probs']
        )
        
        # èåˆä¸“å®¶åŠ¨ä½œ
        if 'expert_actions' in policy_state_dict:
            fusion_result = self.fusion_controller.fuse_expert_actions(
                policy_state_dict['expert_actions'],
                fusion_weights,
                obs
            )
            
            # èåˆæŸå¤±ï¼šé¢„æœŸåŠ¨ä½œä¸å®é™…åŠ¨ä½œçš„å·®å¼‚
            actual_actions = policy_state_dict['actions']
            expected_actions = fusion_result['fused_actions']
            
            fusion_loss = nn.MSELoss()(actual_actions, expected_actions.detach())
            
            # åå‘ä¼ æ’­
            self.fusion_optimizer.zero_grad()
            fusion_loss.backward()
            nn.utils.clip_grad_norm_(self.fusion_controller.parameters(), self.max_grad_norm)
            self.fusion_optimizer.step()
            
            return {
                'fusion_loss': fusion_loss.item(),
                'fusion_quality': fusion_result['fusion_quality'].mean().item(),
                'use_advanced_ratio': fusion_result['use_advanced_ratio'].item()
            }
        
        return {}
    
    def _update_algo_step(self, policy_state_dict, loss_dict):
        """é‡å†™ç®—æ³•æ›´æ–°æ­¥éª¤"""
        # æ ‡å‡†PPOæ›´æ–°
        loss_dict = super()._update_algo_step(policy_state_dict, loss_dict)
        
        # æ ¹æ®å½“å‰é˜¶æ®µå†³å®šæ˜¯å¦æ›´æ–°å¤šæ¨¡æ€ç»„ä»¶
        if self.current_phase == 'pretraining':
            # é¢„è®­ç»ƒé˜¶æ®µï¼šåªæ›´æ–°è¿åŠ¨ç¼–ç å™¨
            encoder_losses = self._update_motion_encoder(policy_state_dict)
            loss_dict.update({f'encoder_{k}': v for k, v in encoder_losses.items()})
            
        elif self.current_phase == 'multimodal':
            # å¤šæ¨¡æ€é˜¶æ®µï¼šæ›´æ–°æ‰€æœ‰ç»„ä»¶
            if self.enable_expert_training:
                encoder_losses = self._update_motion_encoder(policy_state_dict)
                fusion_losses = self._update_fusion_controller(policy_state_dict)
                
                loss_dict.update({f'encoder_{k}': v for k, v in encoder_losses.items()})
                loss_dict.update({f'fusion_{k}': v for k, v in fusion_losses.items()})
        
        return loss_dict
    
    def _check_phase_transition(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢è®­ç»ƒé˜¶æ®µ"""
        if (self.current_phase == 'pretraining' and 
            self.current_learning_iteration >= self.pretraining_iterations):
            
            self.current_phase = 'multimodal'
            print(f"Switching to multimodal training phase at iteration {self.current_learning_iteration}")
            
            # å¯ç”¨èåˆåŠŸèƒ½
            if hasattr(self.env, 'enable_fusion'):
                self.env.enable_fusion = True
    
    def _training_step(self):
        """é‡å†™è®­ç»ƒæ­¥éª¤"""
        # æ£€æŸ¥é˜¶æ®µåˆ‡æ¢
        self._check_phase_transition()
        
        # è°ƒç”¨çˆ¶ç±»è®­ç»ƒæ­¥éª¤
        loss_dict = super()._training_step()
        
        return loss_dict
    
    def _post_epoch_logging(self, log_dict, width=80, pad=40):
        """é‡å†™æ—¥å¿—è®°å½•ï¼Œæ·»åŠ å¤šæ¨¡æ€ä¿¡æ¯"""
        super()._post_epoch_logging(log_dict, width, pad)
        
        # æ·»åŠ å¤šæ¨¡æ€çŠ¶æ€ä¿¡æ¯
        multimodal_info = self.env.get_multimodal_info()
        
        # è®°å½•åˆ°tensorboard
        iteration = log_dict['it']
        for key, value in multimodal_info.items():
            if isinstance(value, (int, float)):
                self.writer.add_scalar(f'Multimodal/{key}', value, iteration)
            elif isinstance(value, np.ndarray):
                for i, v in enumerate(value):
                    self.writer.add_scalar(f'Multimodal/{key}_{i}', v, iteration)
        
        # è®°å½•å½“å‰è®­ç»ƒé˜¶æ®µ
        self.writer.add_scalar('Training/phase', 
                             1 if self.current_phase == 'multimodal' else 0, 
                             iteration)
        
        # æ‰“å°å¤šæ¨¡æ€ä¿¡æ¯
        print(f"\nMultimodal Info:")
        print(f"  Phase: {self.current_phase}")
        print(f"  Fusion Success Rate: {multimodal_info.get('fusion_success_rate', 'N/A')}")
        print(f"  Active Motions: {multimodal_info.get('max_active_motions', 'N/A')}")
        print(f"  Transitioning Envs: {multimodal_info.get('transition_envs', 0)}/{multimodal_info.get('total_envs', 0)}")
    
    def save(self, path, infos=None):
        """ä¿å­˜æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€ç»„ä»¶"""
        save_dict = {
            'model_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'encoder_state_dict': self.motion_encoder.state_dict(),
            'fusion_controller_state_dict': self.fusion_controller.state_dict(),
            'encoder_optimizer_state_dict': self.encoder_optimizer.state_dict(),
            'fusion_optimizer_state_dict': self.fusion_optimizer.state_dict(),
            'iter': self.current_learning_iteration,
            'current_phase': self.current_phase,
            'infos': infos,
        }
        torch.save(save_dict, path)
    
    def load(self, ckpt_path):
        """åŠ è½½æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€ç»„ä»¶"""
        loaded_dict = torch.load(ckpt_path, map_location=self.device)
        
        # åŠ è½½æ ‡å‡†ç»„ä»¶
        self.actor.load_state_dict(loaded_dict['model_state_dict'])
        self.critic.load_state_dict(loaded_dict['critic_state_dict'])
        
        if self.load_optimizer:
            self.actor_optimizer.load_state_dict(loaded_dict['optimizer_state_dict'])
            self.critic_optimizer.load_state_dict(loaded_dict['critic_optimizer_state_dict'])
        
        # åŠ è½½å¤šæ¨¡æ€ç»„ä»¶
        if 'encoder_state_dict' in loaded_dict:
            self.motion_encoder.load_state_dict(loaded_dict['encoder_state_dict'])
        
        if 'fusion_controller_state_dict' in loaded_dict:
            self.fusion_controller.load_state_dict(loaded_dict['fusion_controller_state_dict'])
        
        if self.load_optimizer:
            if 'encoder_optimizer_state_dict' in loaded_dict:
                self.encoder_optimizer.load_state_dict(loaded_dict['encoder_optimizer_state_dict'])
            if 'fusion_optimizer_state_dict' in loaded_dict:
                self.fusion_optimizer.load_state_dict(loaded_dict['fusion_optimizer_state_dict'])
        
        # æ¢å¤è®­ç»ƒé˜¶æ®µ
        if 'current_phase' in loaded_dict:
            self.current_phase = loaded_dict['current_phase']
        
        self.current_learning_iteration = loaded_dict['iter']
        return loaded_dict.get('infos', {})
